Problem 1:

Two objective evaluation methods:

1. ROUGE Score Evaluation method
This method compares the generated text with the reference (edited ending) 
using ROUGE metrics which measure overlap in unigrams, bigrams, and 
longest common subsequences, providing a comprehensive assessment of both 
content and sequence matching.

2. Semantic Consistency Score method
This method evaluates how well the generated ending maintains key terms 
from both the premise and counterfactual by calculating a weighted 
word overlap score between the generated text and the input context.


Subjective Evaluation Criterion:
I choose "Logical Consistency" as the subjective criterion (scale 1-5):
1: Completely illogical given the premise and counterfactual
2: Mostly illogical but has some connection
3: Partially logical but with some inconsistencies
4: Mostly logical with minor inconsistencies
5: Completely logical and consistent with the premise and counterfactual

Problem 2:
In code.

Problem 3:
Overall, the model did not seem to perform better. For both test datapoints, 
when the second example about Niel was introduced in the prompt, it 
significantly skewed the results (the results only talked about Niel's
Australian culture, even though this had nothing to do with the datapoint).
For the first example, the results remained the same as Problem 2. However, 
our ROUGE scores did improve sigificantly, which suggests that there may be 
word overlaps, even though the meanings are inconsistent.

Problem 4:
The model performed much better this time. For all prompts, the responses
logically flowed and related to the prompt better. One of the responses also
flowed better than before. For example, in Problem 3, a response to 
datapoint 1 was 'A dumb kid I believed the eclipse was to blame.', but in 
this problem, the first response was 'As an dumb kid I believed the eclipse 
was to blame.', which makes more sense. Our Consistency Scores were also 
higher. However, we noticed that the second prompt performed worse than the 
first, which suggests that the second prompt's explained chain of thought 
wasn't as cohesive as the first prompt's. 

Problem 5:
The personas did not seem to have much of an effect on the endings generated by the model, as some of the generated endings were still inconsistent and illogical, regardless of the persona used. The ROUGE scores and semantic consistency scores also did not show significant improvement with the use of different personas. One notable exception was the professional persona generating a logical and consistent ending for the first prompt, while the other two personas did not.

Problem 6:
From an objective standpoint, we employed two main approaches: ROUGE scores and semantic consistency scoring. The ROUGE metric provided a good quantitative way to compare generated endings with the reference texts by measuring lexical overlap and sequence matching. However, ROUGE depended too heavily on exact word matches and should the generated endings deviate in wording, it could lead to lower scores even if the meaning was preserved.
Our second objective metric, the semantic consistency score, offered some advantages over ROUGE by being less dependent on exact wording and considering the relationship between different parts of the story. However, the simple token-based implementation seemed to overvalue simple word repetition rather than meaningful thematic consistency.
The subjective evaluation component focused on logical flow, using a 1-5 scale to assess narrative coherence and story logic. This human judgment-based approach proved valuable in capturing nuanced aspects of story quality that automated metrics missed. It allowed for the detection of subtle logical inconsistencies and assessment of overall narrative quality. However, the subjective nature of this evaluation introduces potential inconsistencies, requires significant time investment, and scores varied between different evaluators.
Interestingly, we observed that high ROUGE scores don't always correlate with high subjective scores, suggesting that mathematical similarity to a reference text doesn't necessarily indicate a better story. The semantic consistency metric showed better alignment with subjective evaluation, though still with some notable divergences.