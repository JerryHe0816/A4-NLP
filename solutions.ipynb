{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Initial Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "\u001b[33mDEPRECATION: Loading egg at /Users/jerry/anaconda3/lib/python3.11/site-packages/SCons-4.8.0-py3.11.egg is deprecated. pip 23.3 will enforce this behaviour change. A possible replacement is to use pip for package installation..\u001b[0m\u001b[33m\n",
      "\u001b[0mRequirement already satisfied: transformers in /Users/jerry/anaconda3/lib/python3.11/site-packages (4.32.1)\n",
      "Requirement already satisfied: torch in /Users/jerry/anaconda3/lib/python3.11/site-packages (2.3.0)\n",
      "Requirement already satisfied: sentencepiece in /Users/jerry/anaconda3/lib/python3.11/site-packages (0.2.0)\n",
      "Requirement already satisfied: rouge-score in /Users/jerry/anaconda3/lib/python3.11/site-packages (0.1.2)\n",
      "Requirement already satisfied: nltk in /Users/jerry/anaconda3/lib/python3.11/site-packages (3.8.1)\n",
      "Requirement already satisfied: filelock in /Users/jerry/anaconda3/lib/python3.11/site-packages (from transformers) (3.9.0)\n",
      "Requirement already satisfied: huggingface-hub<1.0,>=0.15.1 in /Users/jerry/anaconda3/lib/python3.11/site-packages (from transformers) (0.15.1)\n",
      "Requirement already satisfied: numpy>=1.17 in /Users/jerry/anaconda3/lib/python3.11/site-packages (from transformers) (1.24.4)\n",
      "Requirement already satisfied: packaging>=20.0 in /Users/jerry/anaconda3/lib/python3.11/site-packages (from transformers) (23.1)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /Users/jerry/anaconda3/lib/python3.11/site-packages (from transformers) (6.0)\n",
      "Requirement already satisfied: regex!=2019.12.17 in /Users/jerry/anaconda3/lib/python3.11/site-packages (from transformers) (2022.7.9)\n",
      "Requirement already satisfied: requests in /Users/jerry/anaconda3/lib/python3.11/site-packages (from transformers) (2.31.0)\n",
      "Requirement already satisfied: tokenizers!=0.11.3,<0.14,>=0.11.1 in /Users/jerry/anaconda3/lib/python3.11/site-packages (from transformers) (0.13.2)\n",
      "Requirement already satisfied: safetensors>=0.3.1 in /Users/jerry/anaconda3/lib/python3.11/site-packages (from transformers) (0.3.2)\n",
      "Requirement already satisfied: tqdm>=4.27 in /Users/jerry/anaconda3/lib/python3.11/site-packages (from transformers) (4.65.0)\n",
      "Requirement already satisfied: typing-extensions>=4.8.0 in /Users/jerry/anaconda3/lib/python3.11/site-packages (from torch) (4.11.0)\n",
      "Requirement already satisfied: sympy in /Users/jerry/anaconda3/lib/python3.11/site-packages (from torch) (1.11.1)\n",
      "Requirement already satisfied: networkx in /Users/jerry/anaconda3/lib/python3.11/site-packages (from torch) (3.1)\n",
      "Requirement already satisfied: jinja2 in /Users/jerry/anaconda3/lib/python3.11/site-packages (from torch) (3.1.2)\n",
      "Requirement already satisfied: fsspec in /Users/jerry/anaconda3/lib/python3.11/site-packages (from torch) (2023.4.0)\n",
      "Requirement already satisfied: absl-py in /Users/jerry/anaconda3/lib/python3.11/site-packages (from rouge-score) (2.1.0)\n",
      "Requirement already satisfied: six>=1.14.0 in /Users/jerry/anaconda3/lib/python3.11/site-packages (from rouge-score) (1.16.0)\n",
      "Requirement already satisfied: click in /Users/jerry/anaconda3/lib/python3.11/site-packages (from nltk) (8.0.4)\n",
      "Requirement already satisfied: joblib in /Users/jerry/anaconda3/lib/python3.11/site-packages (from nltk) (1.2.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /Users/jerry/anaconda3/lib/python3.11/site-packages (from jinja2->torch) (2.1.1)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /Users/jerry/anaconda3/lib/python3.11/site-packages (from requests->transformers) (2.0.4)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /Users/jerry/anaconda3/lib/python3.11/site-packages (from requests->transformers) (3.4)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /Users/jerry/anaconda3/lib/python3.11/site-packages (from requests->transformers) (1.26.16)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /Users/jerry/anaconda3/lib/python3.11/site-packages (from requests->transformers) (2024.8.30)\n",
      "Requirement already satisfied: mpmath>=0.19 in /Users/jerry/anaconda3/lib/python3.11/site-packages (from sympy->torch) (1.3.0)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install transformers torch sentencepiece rouge-score nltk"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Problem 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Setting up model and tokenizer...\n",
      "Loading sample data...\n",
      "\n",
      "Selected Datapoints for Evaluation:\n",
      "\n",
      "Datapoint 1:\n",
      "Premise: The only thing I had of my dad's was his iguana.\n",
      "Initial: Since I couldn't spend time with my dad I played with the lizard.\n",
      "Original Ending: One day there was a solar eclipse. When I got home the lizard was dead. As a dumb kid I believed the eclipse was to blame.\n",
      "Counterfactual: Since I was afraid of lizards, I sold the iguana.\n",
      "Edited Ending: ['One day there was a solar eclipse.', 'When I got home the lizard had found its way back to my house.', 'As a dumb kid I believed the eclipse was to blame.']\n",
      "\n",
      "Datapoint 2:\n",
      "Premise: Tammy hated being a waitress.\n",
      "Initial: Last week she almost quit.\n",
      "Original Ending: Customers walked into her job five minutes before closing. They demanded lots of food and left a poor tip. She cried herself to sleep two days in a row.\n",
      "Counterfactual: Tammy was given the day off today because she knew Tammy was considering quitting.\n",
      "Edited Ending: ['The next day, customers walked into her job five minutes before closing.', 'They demanded lots of food and left a poor tip.', 'She cried herself to sleep two days in a row.']\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import random\n",
    "import torch\n",
    "from transformers import AutoTokenizer, T5ForConditionalGeneration\n",
    "\n",
    "def setup_model():\n",
    "    # Load the model and tokenizer\n",
    "    tokenizer = AutoTokenizer.from_pretrained(\"google/flan-t5-large\")\n",
    "    model = T5ForConditionalGeneration.from_pretrained(\"google/flan-t5-large\")\n",
    "    return tokenizer, model\n",
    "\n",
    "def load_sample_data():\n",
    "    try:\n",
    "        # First, let's try to read the raw content of the file\n",
    "        with open(\"TimeTravel/train_supervised_small.json\", 'r') as f:\n",
    "            content = f.read()\n",
    "            \n",
    "        # Try to parse each line as a separate JSON object\n",
    "        data = []\n",
    "        for line in content.strip().split('\\n'):\n",
    "            if line.strip():  # Skip empty lines\n",
    "                try:\n",
    "                    data.append(json.loads(line))\n",
    "                except json.JSONDecodeError as e:\n",
    "                    print(f\"Warning: Couldn't parse line: {line[:50]}...\")\n",
    "                    continue\n",
    "        \n",
    "        if not data:\n",
    "            raise ValueError(\"No valid JSON objects found in the file\")\n",
    "        \n",
    "        # Select two random datapoints\n",
    "        random.seed(42)  # For reproducibility\n",
    "        sample_points = random.sample(data, min(2, len(data)))\n",
    "        \n",
    "        # Print selected datapoints\n",
    "        print(\"\\nSelected Datapoints for Evaluation:\")\n",
    "        for i, point in enumerate(sample_points):\n",
    "            print(f\"\\nDatapoint {i+1}:\")\n",
    "            print(f\"Premise: {point.get('premise', 'N/A')}\")\n",
    "            print(f\"Initial: {point.get('initial', 'N/A')}\")\n",
    "            print(f\"Original Ending: {point.get('original_ending', 'N/A')}\")\n",
    "            print(f\"Counterfactual: {point.get('counterfactual', 'N/A')}\")\n",
    "            print(f\"Edited Ending: {point.get('edited_ending', 'N/A')}\")\n",
    "        \n",
    "        return sample_points\n",
    "    \n",
    "    except FileNotFoundError:\n",
    "        print(\"Error: Could not find the file 'TimeTravel/train_supervised_small.json'\")\n",
    "        print(\"Please make sure the file exists in the TimeTravel directory\")\n",
    "        return None\n",
    "    except Exception as e:\n",
    "        print(f\"Error loading data: {str(e)}\")\n",
    "        print(\"Could you please share the first few lines of your JSON file?\")\n",
    "        return None\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    print(\"Setting up model and tokenizer...\")\n",
    "    tokenizer, model = setup_model()\n",
    "    print(\"Loading sample data...\")\n",
    "    sample_points = load_sample_data()\n",
    "    \n",
    "    if sample_points is None:\n",
    "        print(\"\\nPlease verify that:\")\n",
    "        print(\"1. The TimeTravel directory exists\")\n",
    "        print(\"2. The file 'train_supervised_small.json' is in the TimeTravel directory\")\n",
    "        print(\"3. The JSON file is properly formatted\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Problem 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Testing datapoint1:\n",
      "ROUGE Scores: {'rouge1': 1.0, 'rouge2': 1.0, 'rougeL': 1.0}\n",
      "Semantic Consistency Score: 0.3667\n",
      "\n",
      "Testing datapoint2:\n",
      "ROUGE Scores: {'rouge1': 1.0, 'rouge2': 1.0, 'rougeL': 1.0}\n",
      "Semantic Consistency Score: 0.23\n"
     ]
    }
   ],
   "source": [
    "from rouge_score import rouge_scorer\n",
    "import numpy as np\n",
    "import string\n",
    "\n",
    "class StoryEvaluator:\n",
    "    def __init__(self):\n",
    "        self.rouge_scorer = rouge_scorer.RougeScorer(['rouge1', 'rouge2', 'rougeL'], use_stemmer=True)\n",
    "    \n",
    "    def evaluate_rouge(self, generated_ending: str, reference_ending: str) -> dict:\n",
    "        \"\"\"\n",
    "        Method 1: ROUGE Score Evaluation\n",
    "        Evaluates the similarity between generated and reference endings using ROUGE metrics\n",
    "        \"\"\"\n",
    "        # Handle list input for reference ending\n",
    "        if isinstance(reference_ending, list):\n",
    "            reference_ending = ' '.join(reference_ending)\n",
    "            \n",
    "        scores = self.rouge_scorer.score(reference_ending, generated_ending)\n",
    "        return {\n",
    "            'rouge1': round(scores['rouge1'].fmeasure, 4),\n",
    "            'rouge2': round(scores['rouge2'].fmeasure, 4),\n",
    "            'rougeL': round(scores['rougeL'].fmeasure, 4)\n",
    "        }\n",
    "    \n",
    "    def simple_tokenize(self, text: str) -> set:\n",
    "        \"\"\"\n",
    "        Simple tokenization function that splits on whitespace and removes punctuation\n",
    "        \"\"\"\n",
    "        # Remove punctuation and convert to lowercase\n",
    "        text = text.lower()\n",
    "        for punct in string.punctuation:\n",
    "            text = text.replace(punct, ' ')\n",
    "        \n",
    "        # Split on whitespace and filter out empty strings\n",
    "        return set(token for token in text.split() if token)\n",
    "    \n",
    "    def evaluate_semantic_consistency(self, generated_ending: str, premise: str, counterfactual: str) -> float:\n",
    "        \"\"\"\n",
    "        Method 2: Semantic Consistency Score\n",
    "        Measures how well the generated ending maintains key elements from premise and counterfactual\n",
    "        \"\"\"\n",
    "        # Tokenize the texts using simple tokenization\n",
    "        premise_tokens = self.simple_tokenize(premise)\n",
    "        counterfactual_tokens = self.simple_tokenize(counterfactual)\n",
    "        generated_tokens = self.simple_tokenize(generated_ending)\n",
    "        \n",
    "        # Calculate overlap with both premise and counterfactual\n",
    "        premise_overlap = len(premise_tokens.intersection(generated_tokens)) / len(premise_tokens) if premise_tokens else 0\n",
    "        counterfactual_overlap = len(counterfactual_tokens.intersection(generated_tokens)) / len(counterfactual_tokens) if counterfactual_tokens else 0\n",
    "        \n",
    "        # Return weighted average of overlaps\n",
    "        return round((0.4 * premise_overlap + 0.6 * counterfactual_overlap), 4)\n",
    "\n",
    "    def subjective_evaluation(self, generated_ending: str, prompt: str) -> int:\n",
    "        \"\"\"\n",
    "        Subjective Evaluation: Logical Flow (1-5 scale)\n",
    "        Evaluates how logically the generated ending follows from the premise and counterfactual\n",
    "        \n",
    "        1: Completely illogical or contradictory\n",
    "        2: Mostly illogical with minor connections\n",
    "        3: Partially logical with some inconsistencies\n",
    "        4: Mostly logical with minor gaps\n",
    "        5: Perfectly logical and consistent\n",
    "        \"\"\"\n",
    "        print(\"\\nSubjective Evaluation (Logical Flow):\")\n",
    "        print(f\"Generated ending: {generated_ending}\")\n",
    "        print(\"\\nPlease rate on a scale of 1-5:\")\n",
    "        print(\"1: Completely illogical or contradictory\")\n",
    "        print(\"2: Mostly illogical with minor connections\")\n",
    "        print(\"3: Partially logical with some inconsistencies\")\n",
    "        print(\"4: Mostly logical with minor gaps\")\n",
    "        print(\"5: Perfectly logical and consistent\")\n",
    "        \n",
    "        while True:\n",
    "            try:\n",
    "                score = int(input(\"Enter score (1-5): \"))\n",
    "                if 1 <= score <= 5:\n",
    "                    return score\n",
    "                print(\"Please enter a number between 1 and 5\")\n",
    "            except ValueError:\n",
    "                print(\"Please enter a valid number\")\n",
    "\n",
    "# Test the evaluator\n",
    "if __name__ == \"__main__\":\n",
    "    evaluator = StoryEvaluator()\n",
    "    \n",
    "    # Test data from your output\n",
    "    test_data = {\n",
    "        'datapoint1': {\n",
    "            'premise': \"The only thing I had of my dad's was his iguana.\",\n",
    "            'initial': \"Since I couldn't spend time with my dad I played with the lizard.\",\n",
    "            'original_ending': \"One day there was a solar eclipse. When I got home the lizard was dead. As a dumb kid I believed the eclipse was to blame.\",\n",
    "            'counterfactual': \"Since I was afraid of lizards, I sold the iguana.\",\n",
    "            'edited_ending': \"One day there was a solar eclipse. When I got home the lizard had found its way back to my house. As a dumb kid I believed the eclipse was to blame.\"\n",
    "        },\n",
    "        'datapoint2': {\n",
    "            'premise': \"Tammy hated being a waitress.\",\n",
    "            'initial': \"Last week she almost quit.\",\n",
    "            'original_ending': \"Customers walked into her job five minutes before closing. They demanded lots of food and left a poor tip. She cried herself to sleep two days in a row.\",\n",
    "            'counterfactual': \"Tammy was given the day off today because she knew Tammy was considering quitting.\",\n",
    "            'edited_ending': \"The next day, customers walked into her job five minutes before closing. They demanded lots of food and left a poor tip. She cried herself to sleep two days in a row.\"\n",
    "        }\n",
    "    }\n",
    "    \n",
    "    # Test both evaluation methods\n",
    "    for dp_name, dp in test_data.items():\n",
    "        print(f\"\\nTesting {dp_name}:\")\n",
    "        \n",
    "        # Test ROUGE scores\n",
    "        rouge_scores = evaluator.evaluate_rouge(\n",
    "            dp['edited_ending'],\n",
    "            dp['edited_ending']  # Using same text for demonstration\n",
    "        )\n",
    "        print(f\"ROUGE Scores: {rouge_scores}\")\n",
    "        \n",
    "        # Test semantic consistency\n",
    "        consistency_score = evaluator.evaluate_semantic_consistency(\n",
    "            dp['edited_ending'],\n",
    "            dp['premise'],\n",
    "            dp['counterfactual'],\n",
    "        )\n",
    "        print(f\"Semantic Consistency Score: {consistency_score}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Evaluating Datapoint 1\n",
      "Premise: The only thing I had of my dad's was his iguana.\n",
      "Counterfactual: Since I was afraid of lizards, I sold the iguana.\n",
      "\n",
      "Prompt 1 generated ending:\n",
      "One day there was a solar eclipse. When I got home the iguana was dead. A dumb kid I believed the eclipse was to blame.\n",
      "\n",
      "Subjective Evaluation (Logical Flow):\n",
      "Generated ending: One day there was a solar eclipse. When I got home the iguana was dead. A dumb kid I believed the eclipse was to blame.\n",
      "\n",
      "Please rate on a scale of 1-5:\n",
      "1: Completely illogical or contradictory\n",
      "2: Mostly illogical with minor connections\n",
      "3: Partially logical with some inconsistencies\n",
      "4: Mostly logical with minor gaps\n",
      "5: Perfectly logical and consistent\n",
      "\n",
      "Prompt 2 generated ending:\n",
      "The iguana was the only thing I had of my dad's.\n",
      "\n",
      "Subjective Evaluation (Logical Flow):\n",
      "Generated ending: The iguana was the only thing I had of my dad's.\n",
      "\n",
      "Please rate on a scale of 1-5:\n",
      "1: Completely illogical or contradictory\n",
      "2: Mostly illogical with minor connections\n",
      "3: Partially logical with some inconsistencies\n",
      "4: Mostly logical with minor gaps\n",
      "5: Perfectly logical and consistent\n",
      "\n",
      "Evaluating Datapoint 2\n",
      "Premise: Tammy hated being a waitress.\n",
      "Counterfactual: Tammy was given the day off today because she knew Tammy was considering quitting.\n",
      "\n",
      "Prompt 1 generated ending:\n",
      "Customers walked into her job five minutes before closing. They demanded lots of food and left a poor tip. She cried herself to sleep.\n",
      "\n",
      "Subjective Evaluation (Logical Flow):\n",
      "Generated ending: Customers walked into her job five minutes before closing. They demanded lots of food and left a poor tip. She cried herself to sleep.\n",
      "\n",
      "Please rate on a scale of 1-5:\n",
      "1: Completely illogical or contradictory\n",
      "2: Mostly illogical with minor connections\n",
      "3: Partially logical with some inconsistencies\n",
      "4: Mostly logical with minor gaps\n",
      "5: Perfectly logical and consistent\n",
      "\n",
      "Prompt 2 generated ending:\n",
      "Tammy decided to quit her job today.\n",
      "\n",
      "Subjective Evaluation (Logical Flow):\n",
      "Generated ending: Tammy decided to quit her job today.\n",
      "\n",
      "Please rate on a scale of 1-5:\n",
      "1: Completely illogical or contradictory\n",
      "2: Mostly illogical with minor connections\n",
      "3: Partially logical with some inconsistencies\n",
      "4: Mostly logical with minor gaps\n",
      "5: Perfectly logical and consistent\n",
      "\n",
      "Final Results:\n",
      "\n",
      "Datapoint 1, Prompt Version 1:\n",
      "Generated: One day there was a solar eclipse. When I got home the iguana was dead. A dumb kid I believed the eclipse was to blame.\n",
      "ROUGE Scores: {'rouge1': 0.7719, 'rouge2': 0.7273, 'rougeL': 0.7719}\n",
      "Consistency Score: 0.4\n",
      "Subjective Score: 2\n",
      "\n",
      "Datapoint 1, Prompt Version 2:\n",
      "Generated: The iguana was the only thing I had of my dad's.\n",
      "ROUGE Scores: {'rouge1': 0.2727, 'rouge2': 0.0, 'rougeL': 0.1818}\n",
      "Consistency Score: 0.7\n",
      "Subjective Score: 4\n",
      "\n",
      "Datapoint 2, Prompt Version 1:\n",
      "Generated: Customers walked into her job five minutes before closing. They demanded lots of food and left a poor tip. She cried herself to sleep.\n",
      "ROUGE Scores: {'rouge1': 0.8571, 'rouge2': 0.8519, 'rougeL': 0.8571}\n",
      "Consistency Score: 0.13\n",
      "Subjective Score: 5\n",
      "\n",
      "Datapoint 2, Prompt Version 2:\n",
      "Generated: Tammy decided to quit her job today.\n",
      "ROUGE Scores: {'rouge1': 0.1538, 'rouge2': 0.0541, 'rougeL': 0.1026}\n",
      "Consistency Score: 0.18\n",
      "Subjective Score: 5\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoTokenizer, T5ForConditionalGeneration\n",
    "from typing import List, Dict\n",
    "\n",
    "class ZeroShotPrompter:\n",
    "    def __init__(self):\n",
    "        self.tokenizer = AutoTokenizer.from_pretrained(\"google/flan-t5-large\")\n",
    "        self.model = T5ForConditionalGeneration.from_pretrained(\"google/flan-t5-large\")\n",
    "        self.evaluator = StoryEvaluator()\n",
    "    \n",
    "    def generate_ending(self, prompt: str) -> str:\n",
    "        \"\"\"Generate a story ending using the model\"\"\"\n",
    "        inputs = self.tokenizer(prompt, return_tensors=\"pt\", max_length=512, truncation=True)\n",
    "        outputs = self.model.generate(\n",
    "            inputs.input_ids,\n",
    "            max_length=150,\n",
    "            num_beams=4,\n",
    "            temperature=0.7,\n",
    "            no_repeat_ngram_size=2\n",
    "        )\n",
    "        return self.tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "    \n",
    "    def evaluate_prompts(self, datapoints: List[Dict]) -> List[Dict]:\n",
    "        # Two different zero-shot prompts\n",
    "        prompts = [\n",
    "            \"\"\"\n",
    "            Write a new ending to this story that follows from the changed fact:\n",
    "            Story beginning: {premise}\n",
    "            Initial: {initial}\n",
    "            Original Ending: {original_ending}\n",
    "            Changed fact: {counterfactual}\n",
    "            New ending:\n",
    "            \"\"\",\n",
    "            \n",
    "            \"\"\"\n",
    "            Rewrite how this story ends, considering that {counterfactual}\n",
    "            Original beginning: {premise}\n",
    "            Write the new ending:\n",
    "            \"\"\"\n",
    "        ]\n",
    "        \n",
    "        results = []\n",
    "        for i, dp in enumerate(datapoints):\n",
    "            print(f\"\\nEvaluating Datapoint {i+1}\")\n",
    "            print(f\"Premise: {dp['premise']}\")\n",
    "            print(f\"Counterfactual: {dp['counterfactual']}\")\n",
    "            \n",
    "            for prompt_idx, prompt_template in enumerate(prompts, 1):\n",
    "                # Format prompt\n",
    "                prompt = prompt_template.format(\n",
    "                    premise=dp['premise'],\n",
    "                    initial=dp['initial'],\n",
    "                    original_ending=dp['original_ending'],\n",
    "                    counterfactual=dp['counterfactual']\n",
    "                )\n",
    "                \n",
    "                # Generate ending\n",
    "                generated_ending = self.generate_ending(prompt)\n",
    "                \n",
    "                # Evaluate using both methods\n",
    "                rouge_scores = self.evaluator.evaluate_rouge(\n",
    "                    generated_ending,\n",
    "                    dp['edited_ending']\n",
    "                )\n",
    "                \n",
    "                consistency_score = self.evaluator.evaluate_semantic_consistency(\n",
    "                    generated_ending,\n",
    "                    dp['premise'],\n",
    "                    dp['counterfactual']\n",
    "                )\n",
    "                \n",
    "                # Get subjective score\n",
    "                print(f\"\\nPrompt {prompt_idx} generated ending:\")\n",
    "                print(generated_ending)\n",
    "                subjective_score = self.evaluator.subjective_evaluation(generated_ending, prompt)\n",
    "                \n",
    "                results.append({\n",
    "                    'datapoint': i+1,\n",
    "                    'prompt_version': prompt_idx,\n",
    "                    'generated_ending': generated_ending,\n",
    "                    'rouge_scores': rouge_scores,\n",
    "                    'consistency_score': consistency_score,\n",
    "                    'subjective_score': subjective_score\n",
    "                })\n",
    "        \n",
    "        return results\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    # Initialize prompter\n",
    "    prompter = ZeroShotPrompter()\n",
    "    \n",
    "    # Use the same test data\n",
    "    test_data = [\n",
    "        {\n",
    "            'premise': \"The only thing I had of my dad's was his iguana.\",\n",
    "            'initial': \"Since I couldn't spend time with my dad I played with the lizard.\",\n",
    "            'original_ending': \"One day there was a solar eclipse. When I got home the lizard was dead. As a dumb kid I believed the eclipse was to blame.\",\n",
    "            'counterfactual': \"Since I was afraid of lizards, I sold the iguana.\",\n",
    "            'edited_ending': \"One day there was a solar eclipse. When I got home the lizard had found its way back to my house. As a dumb kid I believed the eclipse was to blame.\"\n",
    "        },\n",
    "        {\n",
    "            'premise': \"Tammy hated being a waitress.\",\n",
    "            'initial': \"Last week she almost quit.\",\n",
    "            'original_ending': \"Customers walked into her job five minutes before closing. They demanded lots of food and left a poor tip. She cried herself to sleep two days in a row.\",\n",
    "            'counterfactual': \"Tammy was given the day off today because she knew Tammy was considering quitting.\",\n",
    "            'edited_ending': \"The next day, customers walked into her job five minutes before closing. They demanded lots of food and left a poor tip. She cried herself to sleep two days in a row.\"\n",
    "        }\n",
    "    ]\n",
    "    \n",
    "    # Run zero-shot prompting evaluation\n",
    "    results = prompter.evaluate_prompts(test_data)\n",
    "    \n",
    "    # Print final results\n",
    "    print(\"\\nFinal Results:\")\n",
    "    for result in results:\n",
    "        print(f\"\\nDatapoint {result['datapoint']}, Prompt Version {result['prompt_version']}:\")\n",
    "        print(f\"Generated: {result['generated_ending']}\")\n",
    "        print(f\"ROUGE Scores: {result['rouge_scores']}\")\n",
    "        print(f\"Consistency Score: {result['consistency_score']}\")\n",
    "        print(f\"Subjective Score: {result['subjective_score']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Problem 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "class FewShotPrompter:\n",
    "    def __init__(self):\n",
    "        self.tokenizer = AutoTokenizer.from_pretrained(\"google/flan-t5-large\")\n",
    "        self.model = T5ForConditionalGeneration.from_pretrained(\"google/flan-t5-large\")\n",
    "        self.evaluator = StoryEvaluator()\n",
    "        \n",
    "    def generate_ending(self, prompt: str) -> str:\n",
    "        \"\"\"Generate a story ending using the model\"\"\"\n",
    "        inputs = self.tokenizer(prompt, return_tensors=\"pt\", max_length=512, truncation=True)\n",
    "        outputs = self.model.generate(\n",
    "            inputs.input_ids,\n",
    "            max_length=150,\n",
    "            num_beams=4,\n",
    "            temperature=0.7,\n",
    "            no_repeat_ngram_size=2\n",
    "        )\n",
    "        return self.tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "\n",
    "    def evaluate_prompts(self, datapoints: List[Dict]) -> List[Dict]:\n",
    "        prompts = [\n",
    "            \"\"\"\n",
    "            Below is one example of how the story ends when a fact changes in a narrative.\n",
    "\n",
    "            Example 1:\n",
    "            Premise: Ryan was called by his friend to skip work one day.\n",
    "            Initial: He missed his train to work and instead went to the park.\n",
    "            Original Ending: Ryan and his friend played with birds at the park all day. At the end of the day, they left the park and saw Ryan's boss. Ryan got fired.\n",
    "            Counterfactual: But Ryan had an important project at work and went in to finish it.\n",
    "            Edited Ending: [\"After he finished it Ryan and his friend played with birds at the park the rest of the day.\", \"At the end of the day, they left the park and saw Ryan's boss.\", \"Ryan's boss congratulated him on the great work he did on the project.\"], [\"Ryan's friend played with birds at the park all day.\", \"At the end of the day, Ryan's friend left the park and saw Ryan's boss.\", \"Ryan was glad he went to work.\"], [\"Ryan and his friend fell out about it.\", \"At the end of the day, Ryan told his friend's boss.\", \"His friend got fired.\"]\n",
    "\n",
    "            Now, continue this example with the given story:\n",
    "\n",
    "            Story beginning: {premise}\n",
    "            Initial: {initial}\n",
    "            Original Ending: {original_ending}\n",
    "            Changed fact: {counterfactual}\n",
    "            New ending:\n",
    "            \"\"\", \n",
    "            \"\"\"\n",
    "            Here are two examples of how the story changes when a fact is modified in the narrative.\n",
    "\n",
    "            Example 1:\n",
    "            Premise: Ryan was called by his friend to skip work one day.\n",
    "            Initial: He missed his train to work and instead went to the park.\n",
    "            Original Ending: Ryan and his friend played with birds at the park all day. At the end of the day, they left the park and saw Ryan's boss. Ryan got fired.\n",
    "            Counterfactual: But Ryan had an important project at work and went in to finish it.\n",
    "            Edited Ending: [\"After he finished it Ryan and his friend played with birds at the park the rest of the day.\", \"At the end of the day, they left the park and saw Ryan's boss.\", \"Ryan's boss congratulated him on the great work he did on the project.\"], [\"Ryan's friend played with birds at the park all day.\", \"At the end of the day, Ryan's friend left the park and saw Ryan's boss.\", \"Ryan was glad he went to work.\"], [\"Ryan and his friend fell out about it.\", \"At the end of the day, Ryan told his friend's boss.\", \"His friend got fired.\"]\n",
    "\n",
    "            Example 2:\n",
    "            Premise: Neil had been journeying through Asia.\n",
    "            Initial: Now he had worked his way south into Australia.\n",
    "            Original Ending: Neil was so excited to see Australian culture. He was thrilled at the prospect of exotic animals and people! His favorite moment was when he got to feed a baby koala bear.\n",
    "            Counterfactual: But he contracted malaria on a Thai island, and had to be flown home for treatment.\n",
    "            Edited Ending: [\"Neil was upset that he would experience the Thai island culture.\", \"He had been thrilled at the prospect of exotic animals and people.\", \"He was hoping he would be able to feed exotic animals.\"], [\"Neil was so disappointed not to see Australian culture.\", \"He was saddened that he would miss out on seeing the exotic animals and people!\", \"His least favorite moment was leaving Asia to fly back home.\"], [\"Neil was upset about his trip to Asia.\", \"He had been looking forward to seeing exotic animals and people.\", \"He would have loved to feed an exotic animal.\"]\n",
    "\n",
    "            Continue the example with the given story:\n",
    "\n",
    "            Story beginning: {premise}\n",
    "            Initial: {initial}\n",
    "            Original Ending: {original_ending}\n",
    "            Changed fact: {counterfactual}\n",
    "            New ending:\n",
    "            \"\"\"]\n",
    "        \n",
    "        results = []\n",
    "        for i, dp in enumerate(datapoints):\n",
    "            print(f\"\\nEvaluating Datapoint {i+1}\")\n",
    "            print(f\"Premise: {dp['premise']}\")\n",
    "            print(f\"Counterfactual: {dp['counterfactual']}\")\n",
    "            \n",
    "            for prompt_idx, prompt_template in enumerate(prompts, 1):\n",
    "                # Format prompt with examples\n",
    "                prompt = prompt_template.format(\n",
    "                    premise=dp['premise'],\n",
    "                    initial=dp['initial'],\n",
    "                    original_ending=dp['original_ending'],\n",
    "                    counterfactual=dp['counterfactual']\n",
    "                )\n",
    "                \n",
    "                # Generate ending\n",
    "                generated_ending = self.generate_ending(prompt)\n",
    "                \n",
    "                # Evaluate using both methods\n",
    "                rouge_scores = self.evaluator.evaluate_rouge(\n",
    "                    generated_ending,\n",
    "                    dp['edited_ending']\n",
    "                )\n",
    "                \n",
    "                consistency_score = self.evaluator.evaluate_semantic_consistency(\n",
    "                    generated_ending,\n",
    "                    dp['premise'],\n",
    "                    dp['counterfactual']\n",
    "                )\n",
    "                \n",
    "                # Get subjective score\n",
    "                print(f\"\\nPrompt {prompt_idx} generated ending: {generated_ending}\")\n",
    "                subjective_score = self.evaluator.subjective_evaluation(generated_ending, prompt)\n",
    "                \n",
    "                results.append({\n",
    "                    'datapoint': i+1,\n",
    "                    'prompt_version': prompt_idx,\n",
    "                    'generated_ending': generated_ending,\n",
    "                    'rouge_scores': rouge_scores,\n",
    "                    'consistency_score': consistency_score,\n",
    "                    'subjective_score': subjective_score\n",
    "                })\n",
    "        \n",
    "        return results\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Evaluating Datapoint 1\n",
      "Premise: The only thing I had of my dad's was his iguana.\n",
      "Counterfactual: Since I was afraid of lizards, I sold the iguana.\n",
      "\n",
      "Prompt 1 generated ending: One day there was a solar eclipse. When I got home the iguana was dead. A dumb kid I believed the eclipse was to blame.\n",
      "\n",
      "Subjective Evaluation (Logical Flow):\n",
      "Generated ending: One day there was a solar eclipse. When I got home the iguana was dead. A dumb kid I believed the eclipse was to blame.\n",
      "\n",
      "Please rate on a scale of 1-5:\n",
      "1: Completely illogical or contradictory\n",
      "2: Mostly illogical with minor connections\n",
      "3: Partially logical with some inconsistencies\n",
      "4: Mostly logical with minor gaps\n",
      "5: Perfectly logical and consistent\n",
      "\n",
      "Prompt 2 generated ending: [\"Neil was so disappointed not to see Australian culture. He was saddened that he would miss out on seeing the exotic animals and people!\", \"His least favorite moment was leaving Asia to fly back home.\"]\n",
      "\n",
      "Subjective Evaluation (Logical Flow):\n",
      "Generated ending: [\"Neil was so disappointed not to see Australian culture. He was saddened that he would miss out on seeing the exotic animals and people!\", \"His least favorite moment was leaving Asia to fly back home.\"]\n",
      "\n",
      "Please rate on a scale of 1-5:\n",
      "1: Completely illogical or contradictory\n",
      "2: Mostly illogical with minor connections\n",
      "3: Partially logical with some inconsistencies\n",
      "4: Mostly logical with minor gaps\n",
      "5: Perfectly logical and consistent\n",
      "\n",
      "Evaluating Datapoint 2\n",
      "Premise: Tammy hated being a waitress.\n",
      "Counterfactual: Tammy was given the day off today because she knew Tammy was considering quitting.\n",
      "\n",
      "Prompt 1 generated ending: Customers walked into her job five minutes before closing. They demanded lots of food and left a poor tip. She cried herself to sleep.\n",
      "\n",
      "Subjective Evaluation (Logical Flow):\n",
      "Generated ending: Customers walked into her job five minutes before closing. They demanded lots of food and left a poor tip. She cried herself to sleep.\n",
      "\n",
      "Please rate on a scale of 1-5:\n",
      "1: Completely illogical or contradictory\n",
      "2: Mostly illogical with minor connections\n",
      "3: Partially logical with some inconsistencies\n",
      "4: Mostly logical with minor gaps\n",
      "5: Perfectly logical and consistent\n",
      "\n",
      "Prompt 2 generated ending: [\"Neil was so disappointed not to see Australian culture. He was saddened that he would miss out on seeing the exotic animals and people!\", \"His least favorite moment was leaving Asia to fly back home.\"]\n",
      "\n",
      "Subjective Evaluation (Logical Flow):\n",
      "Generated ending: [\"Neil was so disappointed not to see Australian culture. He was saddened that he would miss out on seeing the exotic animals and people!\", \"His least favorite moment was leaving Asia to fly back home.\"]\n",
      "\n",
      "Please rate on a scale of 1-5:\n",
      "1: Completely illogical or contradictory\n",
      "2: Mostly illogical with minor connections\n",
      "3: Partially logical with some inconsistencies\n",
      "4: Mostly logical with minor gaps\n",
      "5: Perfectly logical and consistent\n",
      "\n",
      "Final Results:\n",
      "\n",
      "Datapoint 1, Prompt Version 1:\n",
      "Generated: One day there was a solar eclipse. When I got home the iguana was dead. A dumb kid I believed the eclipse was to blame.\n",
      "ROUGE Scores: {'rouge1': 0.7719, 'rouge2': 0.7273, 'rougeL': 0.7719}\n",
      "Consistency Score: 0.4\n",
      "Subjective Score: 3\n",
      "\n",
      "Datapoint 1, Prompt Version 2:\n",
      "Generated: [\"Neil was so disappointed not to see Australian culture. He was saddened that he would miss out on seeing the exotic animals and people!\", \"His least favorite moment was leaving Asia to fly back home.\"]\n",
      "ROUGE Scores: {'rouge1': 0.209, 'rouge2': 0.0, 'rougeL': 0.1493}\n",
      "Consistency Score: 0.2333\n",
      "Subjective Score: 1\n",
      "\n",
      "Datapoint 2, Prompt Version 1:\n",
      "Generated: Customers walked into her job five minutes before closing. They demanded lots of food and left a poor tip. She cried herself to sleep.\n",
      "ROUGE Scores: {'rouge1': 0.8571, 'rouge2': 0.8519, 'rougeL': 0.8571}\n",
      "Consistency Score: 0.13\n",
      "Subjective Score: 4\n",
      "\n",
      "Datapoint 2, Prompt Version 2:\n",
      "Generated: [\"Neil was so disappointed not to see Australian culture. He was saddened that he would miss out on seeing the exotic animals and people!\", \"His least favorite moment was leaving Asia to fly back home.\"]\n",
      "ROUGE Scores: {'rouge1': 0.0896, 'rouge2': 0.0, 'rougeL': 0.0896}\n",
      "Consistency Score: 0.1\n",
      "Subjective Score: 1\n"
     ]
    }
   ],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    # Initialize prompter\n",
    "    prompter = FewShotPrompter()\n",
    "    \n",
    "    # Use the same test data\n",
    "    test_data = [\n",
    "        {\n",
    "            'premise': \"The only thing I had of my dad's was his iguana.\",\n",
    "            'initial': \"Since I couldn't spend time with my dad I played with the lizard.\",\n",
    "            'original_ending': \"One day there was a solar eclipse. When I got home the lizard was dead. As a dumb kid I believed the eclipse was to blame.\",\n",
    "            'counterfactual': \"Since I was afraid of lizards, I sold the iguana.\",\n",
    "            'edited_ending': \"One day there was a solar eclipse. When I got home the lizard had found its way back to my house. As a dumb kid I believed the eclipse was to blame.\"\n",
    "        },\n",
    "        {\n",
    "            'premise': \"Tammy hated being a waitress.\",\n",
    "            'initial': \"Last week she almost quit.\",\n",
    "            'original_ending': \"Customers walked into her job five minutes before closing. They demanded lots of food and left a poor tip. She cried herself to sleep two days in a row.\",\n",
    "            'counterfactual': \"Tammy was given the day off today because she knew Tammy was considering quitting.\",\n",
    "            'edited_ending': \"The next day, customers walked into her job five minutes before closing. They demanded lots of food and left a poor tip. She cried herself to sleep two days in a row.\"\n",
    "        }\n",
    "    ]\n",
    "    \n",
    "    # Run zero-shot prompting evaluation\n",
    "    results = prompter.evaluate_prompts(test_data)\n",
    "    \n",
    "    # Print final results\n",
    "    print(\"\\nFinal Results:\")\n",
    "    for result in results:\n",
    "        print(f\"\\nDatapoint {result['datapoint']}, Prompt Version {result['prompt_version']}:\")\n",
    "        print(f\"Generated: {result['generated_ending']}\")\n",
    "        print(f\"ROUGE Scores: {result['rouge_scores']}\")\n",
    "        print(f\"Consistency Score: {result['consistency_score']}\")\n",
    "        print(f\"Subjective Score: {result['subjective_score']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Problem 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ChainOfThought:\n",
    "    def __init__(self):\n",
    "        self.tokenizer = AutoTokenizer.from_pretrained(\"google/flan-t5-large\")\n",
    "        self.model = T5ForConditionalGeneration.from_pretrained(\"google/flan-t5-large\")\n",
    "        self.evaluator = StoryEvaluator()\n",
    "        \n",
    "    def generate_ending(self, prompt: str) -> str:\n",
    "        \"\"\"Generate a story ending using the model\"\"\"\n",
    "        inputs = self.tokenizer(prompt, return_tensors=\"pt\", max_length=512, truncation=True)\n",
    "        outputs = self.model.generate(\n",
    "            inputs.input_ids,\n",
    "            max_length=150,\n",
    "            num_beams=4,\n",
    "            temperature=0.7,\n",
    "            no_repeat_ngram_size=2\n",
    "        )\n",
    "        return self.tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "\n",
    "    def evaluate_prompts(self, datapoints: List[Dict]) -> List[Dict]:\n",
    "        prompts = [\n",
    "            \"\"\"\n",
    "            Below is one example of how the story ends when a fact changes in \n",
    "            a narrative.\n",
    "\n",
    "            Example 1:\n",
    "            Premise: Ryan was called by his friend to skip work one day.\n",
    "            Initial: He missed his train to work and instead went to the park.\n",
    "            Original Ending: Ryan and his friend played with birds at the park all day. At the end of the day, they left the park and saw Ryan's boss. Ryan got fired.\n",
    "            Counterfactual: But Ryan had an important project at work and went in to finish it.\n",
    "            \n",
    "            \n",
    "            Let's break this down:\n",
    "            Since Ryan skipped work to go to the park, he could not work on \n",
    "            an important project and had to finish it first before playing.\n",
    "            Thus, an ending could be: After he finished it Ryan and his friend \n",
    "            played with birds at the park the rest of the day.\n",
    "\n",
    "            Now, do the same for this story:\n",
    "\n",
    "            Story beginning: {premise}\n",
    "            Initial: {initial}\n",
    "            Original Ending: {original_ending}\n",
    "            Changed fact: {counterfactual}\n",
    "            New ending:\n",
    "            \"\"\", \n",
    "            \"\"\"\n",
    "            Below is one example of how the story ends when a fact changes in \n",
    "            a narrative.\n",
    "\n",
    "            Example 1:\n",
    "            Premise: Neil had been journeying through Asia.\n",
    "            Initial: Now he had worked his way south into Australia.\n",
    "            Original Ending: Neil was so excited to see Australian culture. \n",
    "            He was thrilled at the prospect of exotic animals and people! His \n",
    "            favorite moment was when he got to feed a baby koala bear.\n",
    "            Counterfactual: But he contracted malaria on a Thai island, and \n",
    "            had to be flown home for treatment.\n",
    "\n",
    "            Let's break this down:\n",
    "            Since Neil contracted malaria, he could no longer explore\n",
    "            Australia. Therefore, an ending could be: Neil was so disappointed \n",
    "            not to see Australian culture.\n",
    "\n",
    "            Continue the example with the given story:\n",
    "\n",
    "            Story beginning: {premise}\n",
    "            Initial: {initial}\n",
    "            Original Ending: {original_ending}\n",
    "            Changed fact: {counterfactual}\n",
    "            New ending:\n",
    "            \"\"\"]\n",
    "        \n",
    "        results = []\n",
    "        for i, dp in enumerate(datapoints):\n",
    "            print(f\"\\nEvaluating Datapoint {i+1}\")\n",
    "            print(f\"Premise: {dp['premise']}\")\n",
    "            print(f\"Counterfactual: {dp['counterfactual']}\")\n",
    "            \n",
    "            for prompt_idx, prompt_template in enumerate(prompts, 1):\n",
    "                # Format prompt with examples\n",
    "                prompt = prompt_template.format(\n",
    "                    premise=dp['premise'],\n",
    "                    initial=dp['initial'],\n",
    "                    original_ending=dp['original_ending'],\n",
    "                    counterfactual=dp['counterfactual']\n",
    "                )\n",
    "                \n",
    "                # Generate ending\n",
    "                generated_ending = self.generate_ending(prompt)\n",
    "                \n",
    "                # Evaluate using both methods\n",
    "                rouge_scores = self.evaluator.evaluate_rouge(\n",
    "                    generated_ending,\n",
    "                    dp['edited_ending']\n",
    "                )\n",
    "                \n",
    "                consistency_score = self.evaluator.evaluate_semantic_consistency(\n",
    "                    generated_ending,\n",
    "                    dp['premise'],\n",
    "                    dp['counterfactual']\n",
    "                )\n",
    "                \n",
    "                # Get subjective score\n",
    "                print(f\"\\nPrompt {prompt_idx} generated ending: {generated_ending}\")\n",
    "                subjective_score = self.evaluator.subjective_evaluation(generated_ending, prompt)\n",
    "                \n",
    "                results.append({\n",
    "                    'datapoint': i+1,\n",
    "                    'prompt_version': prompt_idx,\n",
    "                    'generated_ending': generated_ending,\n",
    "                    'rouge_scores': rouge_scores,\n",
    "                    'consistency_score': consistency_score,\n",
    "                    'subjective_score': subjective_score\n",
    "                })\n",
    "        \n",
    "        return results\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Evaluating Datapoint 1\n",
      "Premise: The only thing I had of my dad's was his iguana.\n",
      "Counterfactual: Since I was afraid of lizards, I sold the iguana.\n",
      "\n",
      "Prompt 1 generated ending: One day there was a solar eclipse. When I got home the iguana was dead. As an dumb kid I believed the eclipse was to blame.\n",
      "\n",
      "Subjective Evaluation (Logical Flow):\n",
      "Generated ending: One day there was a solar eclipse. When I got home the iguana was dead. As an dumb kid I believed the eclipse was to blame.\n",
      "\n",
      "Please rate on a scale of 1-5:\n",
      "1: Completely illogical or contradictory\n",
      "2: Mostly illogical with minor connections\n",
      "3: Partially logical with some inconsistencies\n",
      "4: Mostly logical with minor gaps\n",
      "5: Perfectly logical and consistent\n",
      "\n",
      "Prompt 2 generated ending: One day there was a solar eclipse. When I got home the iguana was dead. A dumb kid I believed the eclipse was to blame.\n",
      "\n",
      "Subjective Evaluation (Logical Flow):\n",
      "Generated ending: One day there was a solar eclipse. When I got home the iguana was dead. A dumb kid I believed the eclipse was to blame.\n",
      "\n",
      "Please rate on a scale of 1-5:\n",
      "1: Completely illogical or contradictory\n",
      "2: Mostly illogical with minor connections\n",
      "3: Partially logical with some inconsistencies\n",
      "4: Mostly logical with minor gaps\n",
      "5: Perfectly logical and consistent\n",
      "\n",
      "Evaluating Datapoint 2\n",
      "Premise: Tammy hated being a waitress.\n",
      "Counterfactual: Tammy was given the day off today because she knew Tammy was considering quitting.\n",
      "\n",
      "Prompt 1 generated ending: Customers walked into her job five minutes before closing. They demanded lots of food and left a poor tip. She cried herself to sleep.\n",
      "\n",
      "Subjective Evaluation (Logical Flow):\n",
      "Generated ending: Customers walked into her job five minutes before closing. They demanded lots of food and left a poor tip. She cried herself to sleep.\n",
      "\n",
      "Please rate on a scale of 1-5:\n",
      "1: Completely illogical or contradictory\n",
      "2: Mostly illogical with minor connections\n",
      "3: Partially logical with some inconsistencies\n",
      "4: Mostly logical with minor gaps\n",
      "5: Perfectly logical and consistent\n",
      "\n",
      "Prompt 2 generated ending: Customers walked into her job five minutes before closing. They demanded lots of food and left a poor tip. She cried herself to sleep.\n",
      "\n",
      "Subjective Evaluation (Logical Flow):\n",
      "Generated ending: Customers walked into her job five minutes before closing. They demanded lots of food and left a poor tip. She cried herself to sleep.\n",
      "\n",
      "Please rate on a scale of 1-5:\n",
      "1: Completely illogical or contradictory\n",
      "2: Mostly illogical with minor connections\n",
      "3: Partially logical with some inconsistencies\n",
      "4: Mostly logical with minor gaps\n",
      "5: Perfectly logical and consistent\n",
      "\n",
      "Final Results:\n",
      "\n",
      "Datapoint 1, Prompt Version 1:\n",
      "Generated: One day there was a solar eclipse. When I got home the iguana was dead. As an dumb kid I believed the eclipse was to blame.\n",
      "ROUGE Scores: {'rouge1': 0.7586, 'rouge2': 0.6786, 'rougeL': 0.7586}\n",
      "Consistency Score: 0.4\n",
      "Subjective Score: 4\n",
      "\n",
      "Datapoint 1, Prompt Version 2:\n",
      "Generated: One day there was a solar eclipse. When I got home the iguana was dead. A dumb kid I believed the eclipse was to blame.\n",
      "ROUGE Scores: {'rouge1': 0.7719, 'rouge2': 0.7273, 'rougeL': 0.7719}\n",
      "Consistency Score: 0.4\n",
      "Subjective Score: 2\n",
      "\n",
      "Datapoint 2, Prompt Version 1:\n",
      "Generated: Customers walked into her job five minutes before closing. They demanded lots of food and left a poor tip. She cried herself to sleep.\n",
      "ROUGE Scores: {'rouge1': 0.8571, 'rouge2': 0.8519, 'rougeL': 0.8571}\n",
      "Consistency Score: 0.13\n",
      "Subjective Score: 4\n",
      "\n",
      "Datapoint 2, Prompt Version 2:\n",
      "Generated: Customers walked into her job five minutes before closing. They demanded lots of food and left a poor tip. She cried herself to sleep.\n",
      "ROUGE Scores: {'rouge1': 0.8571, 'rouge2': 0.8519, 'rougeL': 0.8571}\n",
      "Consistency Score: 0.13\n",
      "Subjective Score: 4\n"
     ]
    }
   ],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    # Initialize prompter\n",
    "    prompter = ChainOfThought()\n",
    "    \n",
    "    # Use the same test data\n",
    "    test_data = [\n",
    "        {\n",
    "            'premise': \"The only thing I had of my dad's was his iguana.\",\n",
    "            'initial': \"Since I couldn't spend time with my dad I played with the lizard.\",\n",
    "            'original_ending': \"One day there was a solar eclipse. When I got home the lizard was dead. As a dumb kid I believed the eclipse was to blame.\",\n",
    "            'counterfactual': \"Since I was afraid of lizards, I sold the iguana.\",\n",
    "            'edited_ending': \"One day there was a solar eclipse. When I got home the lizard had found its way back to my house. As a dumb kid I believed the eclipse was to blame.\"\n",
    "        },\n",
    "        {\n",
    "            'premise': \"Tammy hated being a waitress.\",\n",
    "            'initial': \"Last week she almost quit.\",\n",
    "            'original_ending': \"Customers walked into her job five minutes before closing. They demanded lots of food and left a poor tip. She cried herself to sleep two days in a row.\",\n",
    "            'counterfactual': \"Tammy was given the day off today because she knew Tammy was considering quitting.\",\n",
    "            'edited_ending': \"The next day, customers walked into her job five minutes before closing. They demanded lots of food and left a poor tip. She cried herself to sleep two days in a row.\"\n",
    "        }\n",
    "    ]\n",
    "    \n",
    "    # Run zero-shot prompting evaluation\n",
    "    results = prompter.evaluate_prompts(test_data)\n",
    "    \n",
    "    # Print final results\n",
    "    print(\"\\nFinal Results:\")\n",
    "    for result in results:\n",
    "        print(f\"\\nDatapoint {result['datapoint']}, Prompt Version {result['prompt_version']}:\")\n",
    "        print(f\"Generated: {result['generated_ending']}\")\n",
    "        print(f\"ROUGE Scores: {result['rouge_scores']}\")\n",
    "        print(f\"Consistency Score: {result['consistency_score']}\")\n",
    "        print(f\"Subjective Score: {result['subjective_score']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Problem 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Evaluating Datapoint 1\n",
      "Premise: The only thing I had of my dad's was his iguana.\n",
      "Counterfactual: Since I was afraid of lizards, I sold the iguana.\n",
      "\n",
      "Prompt 1 (Persona: professional) generated ending:\n",
      "One day there was a solar eclipse. When I got home the iguana was dead. A dumb kid I believed the eclipse was to blame.\n",
      "\n",
      "Subjective Evaluation (Logical Flow):\n",
      "Generated ending: One day there was a solar eclipse. When I got home the iguana was dead. A dumb kid I believed the eclipse was to blame.\n",
      "\n",
      "Please rate on a scale of 1-5:\n",
      "1: Completely illogical or contradictory\n",
      "2: Mostly illogical with minor connections\n",
      "3: Partially logical with some inconsistencies\n",
      "4: Mostly logical with minor gaps\n",
      "5: Perfectly logical and consistent\n",
      "\n",
      "Prompt 2 (Persona: logical) generated ending:\n",
      "One day there was a solar eclipse. When I got home the lizard was still alive. As an idiot kid I believed the eclipse was to blame.\n",
      "\n",
      "Subjective Evaluation (Logical Flow):\n",
      "Generated ending: One day there was a solar eclipse. When I got home the lizard was still alive. As an idiot kid I believed the eclipse was to blame.\n",
      "\n",
      "Please rate on a scale of 1-5:\n",
      "1: Completely illogical or contradictory\n",
      "2: Mostly illogical with minor connections\n",
      "3: Partially logical with some inconsistencies\n",
      "4: Mostly logical with minor gaps\n",
      "5: Perfectly logical and consistent\n",
      "\n",
      "Prompt 3 (Persona: empathetic) generated ending:\n",
      "One day there was a solar eclipse. When I got home the lizard was still alive. As the dumb kid I believed the eclipse was to blame.\n",
      "\n",
      "Subjective Evaluation (Logical Flow):\n",
      "Generated ending: One day there was a solar eclipse. When I got home the lizard was still alive. As the dumb kid I believed the eclipse was to blame.\n",
      "\n",
      "Please rate on a scale of 1-5:\n",
      "1: Completely illogical or contradictory\n",
      "2: Mostly illogical with minor connections\n",
      "3: Partially logical with some inconsistencies\n",
      "4: Mostly logical with minor gaps\n",
      "5: Perfectly logical and consistent\n",
      "\n",
      "Evaluating Datapoint 2\n",
      "Premise: Tammy hated being a waitress.\n",
      "Counterfactual: Tammy was given the day off today because she knew Tammy was considering quitting.\n",
      "\n",
      "Prompt 1 (Persona: professional) generated ending:\n",
      "Customers walked into Tammy's job five minutes before closing. They demanded lots of food and left a poor tip. She cried herself to sleep two days.\n",
      "\n",
      "Subjective Evaluation (Logical Flow):\n",
      "Generated ending: Customers walked into Tammy's job five minutes before closing. They demanded lots of food and left a poor tip. She cried herself to sleep two days.\n",
      "\n",
      "Please rate on a scale of 1-5:\n",
      "1: Completely illogical or contradictory\n",
      "2: Mostly illogical with minor connections\n",
      "3: Partially logical with some inconsistencies\n",
      "4: Mostly logical with minor gaps\n",
      "5: Perfectly logical and consistent\n",
      "\n",
      "Prompt 2 (Persona: logical) generated ending:\n",
      "Customers walked into her job five minutes before closing. They demanded lots of food and left a poor tip. Tammy cried herself to sleep two days in one row.\n",
      "\n",
      "Subjective Evaluation (Logical Flow):\n",
      "Generated ending: Customers walked into her job five minutes before closing. They demanded lots of food and left a poor tip. Tammy cried herself to sleep two days in one row.\n",
      "\n",
      "Please rate on a scale of 1-5:\n",
      "1: Completely illogical or contradictory\n",
      "2: Mostly illogical with minor connections\n",
      "3: Partially logical with some inconsistencies\n",
      "4: Mostly logical with minor gaps\n",
      "5: Perfectly logical and consistent\n",
      "\n",
      "Prompt 3 (Persona: empathetic) generated ending:\n",
      "Customers walked into Tammy's job five minutes before closing. They demanded lots of food and left a poor tip. She cried herself to sleep two days.\n",
      "\n",
      "Subjective Evaluation (Logical Flow):\n",
      "Generated ending: Customers walked into Tammy's job five minutes before closing. They demanded lots of food and left a poor tip. She cried herself to sleep two days.\n",
      "\n",
      "Please rate on a scale of 1-5:\n",
      "1: Completely illogical or contradictory\n",
      "2: Mostly illogical with minor connections\n",
      "3: Partially logical with some inconsistencies\n",
      "4: Mostly logical with minor gaps\n",
      "5: Perfectly logical and consistent\n",
      "\n",
      "Final Results:\n",
      "\n",
      "Datapoint 1, Persona: professional:\n",
      "Generated: One day there was a solar eclipse. When I got home the iguana was dead. A dumb kid I believed the eclipse was to blame.\n",
      "ROUGE Scores: {'rouge1': 0.7719, 'rouge2': 0.7273, 'rougeL': 0.7719}\n",
      "Consistency Score: 0.4\n",
      "Subjective Score: 3\n",
      "\n",
      "Datapoint 1, Persona: logical:\n",
      "Generated: One day there was a solar eclipse. When I got home the lizard was still alive. As an idiot kid I believed the eclipse was to blame.\n",
      "ROUGE Scores: {'rouge1': 0.7458, 'rouge2': 0.6667, 'rougeL': 0.7458}\n",
      "Consistency Score: 0.3\n",
      "Subjective Score: 2\n",
      "\n",
      "Datapoint 1, Persona: empathetic:\n",
      "Generated: One day there was a solar eclipse. When I got home the lizard was still alive. As the dumb kid I believed the eclipse was to blame.\n",
      "ROUGE Scores: {'rouge1': 0.7797, 'rouge2': 0.7018, 'rougeL': 0.7797}\n",
      "Consistency Score: 0.3\n",
      "Subjective Score: 2\n",
      "\n",
      "Datapoint 2, Persona: professional:\n",
      "Generated: Customers walked into Tammy's job five minutes before closing. They demanded lots of food and left a poor tip. She cried herself to sleep two days.\n",
      "ROUGE Scores: {'rouge1': 0.8475, 'rouge2': 0.807, 'rougeL': 0.8475}\n",
      "Consistency Score: 0.26\n",
      "Subjective Score: 4\n",
      "\n",
      "Datapoint 2, Persona: logical:\n",
      "Generated: Customers walked into her job five minutes before closing. They demanded lots of food and left a poor tip. Tammy cried herself to sleep two days in one row.\n",
      "ROUGE Scores: {'rouge1': 0.8852, 'rouge2': 0.8136, 'rougeL': 0.8852}\n",
      "Consistency Score: 0.21\n",
      "Subjective Score: 4\n",
      "\n",
      "Datapoint 2, Persona: empathetic:\n",
      "Generated: Customers walked into Tammy's job five minutes before closing. They demanded lots of food and left a poor tip. She cried herself to sleep two days.\n",
      "ROUGE Scores: {'rouge1': 0.8475, 'rouge2': 0.807, 'rougeL': 0.8475}\n",
      "Consistency Score: 0.26\n",
      "Subjective Score: 4\n"
     ]
    }
   ],
   "source": [
    "class PersonaPrompter:\n",
    "    def __init__(self):\n",
    "        self.tokenizer = AutoTokenizer.from_pretrained(\"google/flan-t5-large\")\n",
    "        self.model = T5ForConditionalGeneration.from_pretrained(\"google/flan-t5-large\")\n",
    "        self.evaluator = StoryEvaluator()\n",
    "        \n",
    "    def generate_ending(self, prompt: str) -> str:\n",
    "        \"\"\"Generate a story ending using the model\"\"\"\n",
    "        inputs = self.tokenizer(prompt, return_tensors=\"pt\", max_length=512, truncation=True)\n",
    "        outputs = self.model.generate(\n",
    "            inputs.input_ids,\n",
    "            max_length=150,\n",
    "            num_beams=4,\n",
    "            temperature=0.7,\n",
    "            no_repeat_ngram_size=2\n",
    "        )\n",
    "        return self.tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "\n",
    "    def evaluate_prompts(self, datapoints: List[Dict]) -> List[Dict]:\n",
    "        # Three different persona-based prompts\n",
    "        prompts = [\n",
    "            \"\"\"\n",
    "            You are a professional storyteller with years of experience in creative writing.\n",
    "            Using your expertise in crafting compelling narratives, please rewrite the ending \n",
    "            of this story considering the changed circumstance:\n",
    "\n",
    "            Story beginning: {premise}\n",
    "            Initial: {initial}\n",
    "            Original Ending: {original_ending}\n",
    "            Changed fact: {counterfactual}\n",
    "\n",
    "            As an expert storyteller, create a new ending that naturally flows from the changed fact:\n",
    "            \"\"\",\n",
    "            \n",
    "            \"\"\"\n",
    "            You are a logical detective who specializes in cause-and-effect analysis.\n",
    "            Given a change in the story's circumstances, determine the most probable outcome:\n",
    "\n",
    "            Story beginning: {premise}\n",
    "            Initial: {initial}\n",
    "            Original Ending: {original_ending}\n",
    "            Changed fact: {counterfactual}\n",
    "\n",
    "            Based on your analytical expertise, deduce the most logical ending:\n",
    "            \"\"\",\n",
    "            \n",
    "            \"\"\"\n",
    "            You are an empathetic counselor who understands human emotions and motivations.\n",
    "            Consider how the characters' feelings and reactions would change given this new situation:\n",
    "\n",
    "            Story beginning: {premise}\n",
    "            Initial: {initial}\n",
    "            Original Ending: {original_ending}\n",
    "            Changed fact: {counterfactual}\n",
    "\n",
    "            Using your understanding of human psychology, write a new ending that reflects the emotional impact:\n",
    "            \"\"\"\n",
    "        ]\n",
    "        \n",
    "        results = []\n",
    "        for i, dp in enumerate(datapoints):\n",
    "            print(f\"\\nEvaluating Datapoint {i+1}\")\n",
    "            print(f\"Premise: {dp['premise']}\")\n",
    "            print(f\"Counterfactual: {dp['counterfactual']}\")\n",
    "            \n",
    "            for prompt_idx, prompt_template in enumerate(prompts, 1):\n",
    "                # Format prompt\n",
    "                prompt = prompt_template.format(\n",
    "                    premise=dp['premise'],\n",
    "                    initial=dp['initial'],\n",
    "                    original_ending=dp['original_ending'],\n",
    "                    counterfactual=dp['counterfactual']\n",
    "                )\n",
    "                \n",
    "                # Generate ending\n",
    "                generated_ending = self.generate_ending(prompt)\n",
    "                \n",
    "                # Evaluate using both methods\n",
    "                rouge_scores = self.evaluator.evaluate_rouge(\n",
    "                    generated_ending,\n",
    "                    dp['edited_ending']\n",
    "                )\n",
    "                \n",
    "                consistency_score = self.evaluator.evaluate_semantic_consistency(\n",
    "                    generated_ending,\n",
    "                    dp['premise'],\n",
    "                    dp['counterfactual']\n",
    "                )\n",
    "                \n",
    "                # Get subjective score\n",
    "                print(f\"\\nPrompt {prompt_idx} (Persona: {prompt_template.split()[3]}) generated ending:\")\n",
    "                print(generated_ending)\n",
    "                subjective_score = self.evaluator.subjective_evaluation(generated_ending, prompt)\n",
    "                \n",
    "                results.append({\n",
    "                    'datapoint': i+1,\n",
    "                    'prompt_version': prompt_idx,\n",
    "                    'persona': prompt_template.split()[3],  # Extract persona from prompt\n",
    "                    'generated_ending': generated_ending,\n",
    "                    'rouge_scores': rouge_scores,\n",
    "                    'consistency_score': consistency_score,\n",
    "                    'subjective_score': subjective_score\n",
    "                })\n",
    "        \n",
    "        return results\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    # Initialize prompter\n",
    "    prompter = PersonaPrompter()\n",
    "    \n",
    "    # Use the same test data\n",
    "    test_data = [\n",
    "        {\n",
    "            'premise': \"The only thing I had of my dad's was his iguana.\",\n",
    "            'initial': \"Since I couldn't spend time with my dad I played with the lizard.\",\n",
    "            'original_ending': \"One day there was a solar eclipse. When I got home the lizard was dead. As a dumb kid I believed the eclipse was to blame.\",\n",
    "            'counterfactual': \"Since I was afraid of lizards, I sold the iguana.\",\n",
    "            'edited_ending': \"One day there was a solar eclipse. When I got home the lizard had found its way back to my house. As a dumb kid I believed the eclipse was to blame.\"\n",
    "        },\n",
    "        {\n",
    "            'premise': \"Tammy hated being a waitress.\",\n",
    "            'initial': \"Last week she almost quit.\",\n",
    "            'original_ending': \"Customers walked into her job five minutes before closing. They demanded lots of food and left a poor tip. She cried herself to sleep two days in a row.\",\n",
    "            'counterfactual': \"Tammy was given the day off today because she knew Tammy was considering quitting.\",\n",
    "            'edited_ending': \"The next day, customers walked into her job five minutes before closing. They demanded lots of food and left a poor tip. She cried herself to sleep two days in a row.\"\n",
    "        }\n",
    "    ]\n",
    "    \n",
    "    # Run persona-based prompting evaluation\n",
    "    results = prompter.evaluate_prompts(test_data)\n",
    "    \n",
    "    # Print final results\n",
    "    print(\"\\nFinal Results:\")\n",
    "    for result in results:\n",
    "        print(f\"\\nDatapoint {result['datapoint']}, Persona: {result['persona']}:\")\n",
    "        print(f\"Generated: {result['generated_ending']}\")\n",
    "        print(f\"ROUGE Scores: {result['rouge_scores']}\")\n",
    "        print(f\"Consistency Score: {result['consistency_score']}\")\n",
    "        print(f\"Subjective Score: {result['subjective_score']}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
